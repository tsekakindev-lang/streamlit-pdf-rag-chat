# ğŸ“š Local Chinese PDF RAG Chatbot (Streamlit + Chroma + Ollama)

A local PDF Q&A app that lets you:
- ğŸ“„ Upload one or more PDFs  
- ğŸ§  Build a local vector index (Chroma)  
- ğŸ’¬ Ask questions with Retrieval-Augmented Generation (RAG)  
- ğŸ” View retrieved source snippets + page numbers  

---

## âœ¨ Features

- ğŸ“ Multiple PDF upload
- ğŸ§± One-click re-index
- ğŸ§¹ Chat history + â€œclear chatâ€
- ğŸ§¬ Local embeddings (Sentence-Transformers style)
- ğŸ¤– Local LLM via Ollama

---

## ğŸ“¸ Screenshot

![App Screenshot](docs/screenshot.png)

---

## ğŸ—‚ï¸ Project structure

âœ… Recommended repo contents (commit these):
- `rag_web.py`
- `requirements.txt`
- `README.md`
- `.gitignore`

ğŸš« Local/runtime contents (do **not** commit):
- `bge-large-zh-v1.5/` (embedding model files)
- `chroma_db_1/` (local vector DB)
- `uploaded_pdfs/` (your PDFs)

---

## âœ… Requirements

ğŸ Python 3.10+ (3.11 recommended)

ğŸ§° Ollama installed and running

ğŸ“¦ An Ollama model pulled locally (default in code: `deepseek-r1:14b`)

ğŸ§  A local embedding model folder at `./bge-large-zh-v1.5`

---

## âš™ï¸ Setup

### 1) ğŸ§ª Create and activate a virtual environment
   
Windows (PowerShell)
```bash
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

macOS / Linux
```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 2) ğŸ“¥ Install Python dependencies
   
```bash
pip install -r requirements.txt
```

### 3) ğŸ§  Prepare the embedding model folder (`bge-large-zh-v1.5/`)

Place your embedding model files under:
```
./bge-large-zh-v1.5/
```

Two common ways:

âœ… Option A: use a pre-downloaded folder

Copy the model directory into the project root and rename it to `bge-large-zh-v1.5`.

â¬‡ï¸ Option B: download via `huggingface_hub` (example snippet)

```Python
from huggingface_hub import snapshot_download

snapshot_download(
    repo_id="sentence-transformers/bge-large-zh-v1.5",
    local_dir="./bge-large-zh-v1.5",
    local_dir_use_symlinks=False,
)
print("Done")
```

### 4) ğŸ¤– Setup Ollama and pull an LLM

Make sure Ollama is running, then pull the model used in `rag_web.py`:

```bash
ollama pull deepseek-r1:14b
```

You can switch to another model by editing in `rag_web.py`:

```bash
OLLAMA_MODEL = "deepseek-r1:14b"
```

---

## â–¶ï¸ Run

```bash
streamlit run rag_web.py
```

Open the local Streamlit URL shown in the terminal. ğŸŒ

---

## ğŸ§­ How to use

### 1) In the left sidebar:

  - ğŸ“„ Upload one or more PDFs

  - ğŸ—‚ï¸ Click â€œğŸ—‚ï¸ é‡æ–°ç´¢å¼•æ–‡æ¡£â€ to build the index

### 2) ğŸ’¬ Start chatting in the main panel

### 3) ğŸ“‘ Expand â€œğŸ“‘ æŸ¥çœ‹å¼•ç”¨æ¥æºâ€ to see retrieved sources and page numbers


## ğŸ› ï¸ Configuration (inside rag_web.py)

```python
ğŸ”§ Key settings you may want to adjust:
```

### ğŸ§  Embeddings model path:

```python
EMBED_MODEL_PATH = "./bge-large-zh-v1.5"
```

### ğŸ—ƒï¸ Chroma persistence:

```python
CHROMA_DIR = "./chroma_db_1"
COLLECTION_NAME = "rag_collection"
```

### âœ‚ï¸ Chunking / retrieval:

```python
CHUNK_SIZE = 600
CHUNK_OVERLAP = 100
TOP_K = 5
```

### ğŸšï¸ Optional similarity score threshold:

```python
USE_SCORE_THRESHOLD = False
SCORE_THRESHOLD = 0.65
```

---

## ğŸ§¯ Troubleshooting

### â— â€œPlease upload PDF and reindexâ€ keeps showing

  - âœ… Ensure you clicked â€œé‡æ–°ç´¢å¼•æ–‡æ¡£â€ at least once

  - âœ… Or ensure ./chroma_db_1/ already exists and is not empty

### â— Ollama errors / model not found

  - âœ… Confirm Ollama is running

  - âœ… Confirm the model exists locally:

  ```bash
  ollama list
  ```

### â— Windows: folder/lock issues during reindex

The app deletes and recreates the collection (not the whole folder) to reduce file-lock issues. If you still see locks:

  - ğŸ”’ Close other programs reading chroma_db_1/

  - ğŸ” Restart the Streamlit app

### â— Answers ignore the PDF content

  - ğŸ” Increase TOP_K

  - âœ‚ï¸ Reduce CHUNK_SIZE if passages are long

  - ğŸšï¸ Turn on USE_SCORE_THRESHOLD only after confirming retrieval works well

---

## ğŸªª License

This project is released under the MIT License. See LICENSE for details.

















