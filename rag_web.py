# pip install --upgrade pip
# pip install streamlit pymupdf langchain langchain-community langchain-text-splitters langchain-huggingface sentence-transformers

import os
import streamlit as st

from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama import OllamaLLM

from langchain_core.prompts import PromptTemplate
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_classic.chains.retrieval import create_retrieval_chain

# ======================
# Configuration
# ======================
st.set_page_config(page_title="æœ¬åœ°ä¸­æ–‡ RAG èŠå¤©æœºå™¨äºº", layout="wide")
st.title("ğŸ“š æœ¬åœ° RAG èŠå¤©æœºå™¨äºº")

CHUNK_SIZE = 600
CHUNK_OVERLAP = 100
TOP_K = 5

# å»ºè®®ï¼šnormalize_embeddings=True æ—¶ï¼Œscore_threshold å¸¸è§è¦ 0.6~0.75
USE_SCORE_THRESHOLD = False  # å…ˆå…³æ‰æœ€ç¨³ï¼Œæƒ³å¼€å†æ”¹ True
SCORE_THRESHOLD = 0.65

OLLAMA_MODEL = "deepseek-r1:14b"      # æ”¹æˆä½ æœ¬æœº ollama å·²æ‹‰å–çš„æ¨¡å‹å
CHROMA_DIR = "./chroma_db_1"
PDF_DIR = "./uploaded_pdfs"
EMBED_MODEL_PATH = "./bge-large-zh-v1.5"  # æœ¬åœ° bge æ¨¡å‹è·¯å¾„

COLLECTION_NAME = "rag_collection"

# ======================
# Helpers (cache)
# ======================

@st.cache_resource
def load_embeddings():
    model_path = os.path.abspath(EMBED_MODEL_PATH)
    return HuggingFaceEmbeddings(
        model_name=model_path,
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )

@st.cache_resource
def load_llm():
    return OllamaLLM(model=OLLAMA_MODEL)

def ensure_dirs():
    os.makedirs(PDF_DIR, exist_ok=True)
    os.makedirs(CHROMA_DIR, exist_ok=True)

def build_or_load_vectorstore(embeddings: HuggingFaceEmbeddings):
    if os.path.exists(CHROMA_DIR) and os.listdir(CHROMA_DIR):
        return Chroma(
            persist_directory=CHROMA_DIR,
            embedding_function=embeddings,
            collection_name=COLLECTION_NAME,
        )
    return None


def make_retriever(vectorstore: Chroma):
    if USE_SCORE_THRESHOLD:
        return vectorstore.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={"k": TOP_K, "score_threshold": SCORE_THRESHOLD},
        )
    return vectorstore.as_retriever(search_kwargs={"k": TOP_K})

def history_to_text(messages):
    """
    æŠŠ st.session_state.messages è½¬ä¸ºè¾ƒå¹²å‡€çš„æ–‡æœ¬ history
    """
    lines = []
    for m in messages:
        role = "ç”¨æˆ·" if m["role"] == "user" else "åŠ©æ‰‹"
        lines.append(f"{role}: {m['content']}")
    return "\n".join(lines[-12:])  # åªå–æœ€è¿‘ 12 è½®ï¼Œé˜²æ­¢ prompt è¿‡é•¿

def clear_chat():
    st.session_state.messages = []
    # Optional: also clear any UI/state flags you use
    # st.session_state.vectorstore_ready = False
    st.rerun()

# ======================
# Sidebar: PDF Upload & Indexing
# ======================
ensure_dirs()

with st.sidebar:
    st.header("æ–‡æ¡£ç®¡ç†")

    uploaded_files = st.file_uploader(
        "ä¸Šä¼  PDF æ–‡ä»¶ï¼ˆæ”¯æŒå¤šä¸ªï¼‰",
        type=["pdf"],
        accept_multiple_files=True,
    )

    reindex = st.button("ğŸ—‚ï¸ é‡æ–°ç´¢å¼•æ–‡æ¡£")

    st.divider()
    if st.button("ğŸ§¹ æ¸…ç©ºèŠå¤©è®°å½•", use_container_width=True):
        clear_chat()

    if reindex:
        if not uploaded_files and not os.listdir(PDF_DIR):
            st.warning("è¯·å…ˆä¸Šä¼  PDF æ–‡ä»¶")
            st.stop()

        with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£å¹¶æ„å»ºç´¢å¼•..."):
            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆè¿½åŠ /è¦†ç›–åŒåï¼‰
            for f in uploaded_files or []:
                save_path = os.path.join(PDF_DIR, f.name)
                with open(save_path, "wb") as out:
                    out.write(f.getbuffer())

            # è¯»å–ç›®å½•ä¸­æ‰€æœ‰ pdf
            docs = []
            for filename in os.listdir(PDF_DIR):
                if filename.lower().endswith(".pdf"):
                    loader = PyMuPDFLoader(os.path.join(PDF_DIR, filename))
                    docs.extend(loader.load())

            if not docs:
                st.error("æœªåŠ è½½åˆ°ä»»ä½•æ–‡æ¡£å†…å®¹ï¼ˆPDF ä¸ºç©ºæˆ–è¯»å–å¤±è´¥ï¼‰")
                st.stop()

            splitter = RecursiveCharacterTextSplitter(
                chunk_size=CHUNK_SIZE,
                chunk_overlap=CHUNK_OVERLAP,
            )
            chunks = splitter.split_documents(docs)
            st.info(f"åˆ†å‰²ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å—")

            embeddings = load_embeddings()

            import chromadb

            # åªåˆªåŒä¸€å€‹ collectionï¼ˆä¸åˆªè³‡æ–™å¤¾ï¼Œé¿å… WinError 32ï¼‰
            client = chromadb.PersistentClient(path=CHROMA_DIR)
            try:
                client.delete_collection(COLLECTION_NAME)
            except Exception:
                pass

            # å»ºæ–°ç´¢å¼•ï¼ˆç”¨åŒä¸€å€‹ collection_nameï¼‰
            vectorstore = Chroma.from_documents(
                documents=chunks,
                embedding=embeddings,
                persist_directory=CHROMA_DIR,
                collection_name=COLLECTION_NAME,
            )

            st.session_state.vectorstore_ready = True
            st.success("âœ… ç´¢å¼•å®Œæˆï¼å¯ä»¥å¼€å§‹èŠå¤©äº†")


# ======================
# Build Chain (LCEL)
# ======================
embeddings = load_embeddings()
llm = load_llm()

vectorstore = build_or_load_vectorstore(embeddings)

if vectorstore is not None:
    retriever = make_retriever(vectorstore)

#- å…è®¸ä¸­è‹±æ–‡æ··åˆï¼Œä¼˜å…ˆ
    PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨ä½†è¡¨è¾¾è‡ªç„¶çš„åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æ ¹æ®ã€ä¸Šä¸‹æ–‡ã€‘å›ç­”é—®é¢˜ã€‚
- å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥å›ç­”ï¼šâ€œæ–‡æ¡£ä¸­æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚â€
- ç”¨ä¸­æ–‡å›ç­”ã€‚

ã€ä¸Šä¸‹æ–‡ã€‘
{context}

ã€å¯¹è¯å†å²ã€‘
{chat_history}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{input}

ã€å›ç­”ã€‘
""".strip()

    prompt = PromptTemplate(
        template=PROMPT_TEMPLATE,
        input_variables=["context", "chat_history", "input"],
    )

    # stuff chainï¼šæŠŠæ£€ç´¢åˆ°çš„ docs å¡è¿› prompt çš„ {context}
    combine_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)

    # retrieval chainï¼šè¾“å…¥ {"input": "..."}ï¼Œå†…éƒ¨å…ˆæ£€ç´¢ï¼Œå†æŠŠ docs äº¤ç»™ combine_chain
    rag_chain = create_retrieval_chain(retriever, combine_chain)

    st.session_state.rag_chain = rag_chain
else:
    st.session_state.rag_chain = None


# ======================
# Chat Interface
# ======================
if "messages" not in st.session_state:
    st.session_state.messages = []

# å±•ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if st.session_state.rag_chain is None:
    st.info("ğŸ‘† è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼  PDF å¹¶ç‚¹å‡»â€œé‡æ–°ç´¢å¼•æ–‡æ¡£â€ï¼Œæˆ–ç¡®è®¤æœ¬åœ°å·²æœ‰ç´¢å¼•ç›®å½•ã€‚")
    st.stop()

# è¾“å…¥æ¡†
user_q = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...")

if user_q:
    st.session_state.messages.append({"role": "user", "content": user_q})
    with st.chat_message("user"):
        st.markdown(user_q)

    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            chat_history_text = history_to_text(st.session_state.messages[:-1])

            # LCEL chain invoke
            result = st.session_state.rag_chain.invoke(
                {
                    "input": user_q,
                    "chat_history": chat_history_text,
                }
            )

            # create_retrieval_chain çš„å…¸å‹è¾“å‡ºï¼š
            # result["answer"] -> æ¨¡å‹å›ç­”
            # result["context"] -> æ£€ç´¢åˆ°çš„ Document åˆ—è¡¨
            answer = result.get("answer", "").strip()
            st.markdown(answer)

            # å¼•ç”¨æ¥æº
            with st.expander("ğŸ“‘ æŸ¥çœ‹å¼•ç”¨æ¥æº"):
                ctx_docs = result.get("context", []) or []
                if not ctx_docs:
                    st.write("ï¼ˆæœ¬æ¬¡æœªæ£€ç´¢åˆ°åŒ¹é…ç‰‡æ®µï¼‰")
                else:
                    for i, doc in enumerate(ctx_docs, start=1):
                        src = doc.metadata.get("source", "æœªçŸ¥æ–‡ä»¶")
                        page = doc.metadata.get("page", "?")
                        st.write(f"**æ¥æº {i}**ï¼š{src}ï¼ˆç¬¬ {page} é¡µï¼‰")
                        st.write(doc.page_content[:600] + ("..." if len(doc.page_content) > 600 else ""))

    st.session_state.messages.append({"role": "assistant", "content": answer})
